[{"content":"There are two main ways geospatial data are stored\nrasters, where each \u0026lsquo;pixel\u0026rsquo; stores data values. This corresponds to files such as geotiffs. vectors, where information is stored more like a table form, and each row will have a \u0026lsquo;geometry\u0026rsquo; field which stores information that allows you to recreate a geographical feature, such as a point , a polygon etc. It might also store the coordinate reference system so you can correctly place the shapes on a map This set of notes corresponds to handling raster data. For this, we will make use of the rasterio library for manipulating the data, as well as pyproj for coordinate transforms.\n(You can access a running version of this notebook on colab here)\nSetup For this demo, I will be using a population dataset from NASA \u0026ldquo;Gridded Population of the World, Version 4 (GPWv4): Population Count Adjusted to Match 2015 Revision of UN WPP Country Totals\u0026rdquo;\n!pip install rasterio pyproj --quiet import rasterio import pyproj import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline peak at data: asc is much like a text file and we can use conventional text handling tools and a bit of pandas to parse it, but it is much less mistake prone if we use rasterio to handle it\n!cat gpw_v4_population_count_adjusted_to_2015_unwpp_country_totals_rev11_2020_1_deg.asc | head -6 ncols 360 nrows 180 xllcorner -180 yllcorner -90 cellsize 1.0000000000001 NODATA_value -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 ... !cat gpw_v4_population_count_adjusted_to_2015_unwpp_country_totals_rev11_2020_1_deg.prj | head -10 Projection GEOGRAPHIC Datum WGS84 Spheroid WGS84 Units DD Zunits NO Parameters parsing the data For this example, we would like to convert the raster to a tabular form we can store in a \u0026lsquo;conventional\u0026rsquo; database/csv file. For this, we will do these tasks:\nread the file into memory\nfilter out any pixels/cells with no data\nconvert the data into a \u0026rsquo;long\u0026rsquo; table with the format longitude | latitude | population | obtain the lat/lng values in web mercator coordinates\nRead the file: note that rasterio can also open files on certain cloud providers, such as GCS/S3 etc, just pass it the relevant filepath in gs://, s3:// format\ndataset = rasterio.open(\u0026#34;gpw_v4_population_count_adjusted_to_2015_unwpp_country_totals_rev11_2020_1_deg.asc\u0026#34;) the dataset object holds metadata:\n# extent of the data and crs -- note we are getting the CRS info from the .prj file also supplied dataset.bounds, dataset.crs (BoundingBox(left=-180.0, bottom=-90.0, right=180.0, top=90.0), CRS.from_epsg(4326)) The actual values of the pixels are in the 1st \u0026lsquo;band\u0026rsquo;. There can be more than 1 band\u0026ndash; for example satellite imagery can comes in \u0026gt; 10 bands. We can either read all the bands in at once, or one by one. (Note that the indexing starts with 1 for the bands if reading in one by one)\ndata= dataset.read() data.shape (1, 180, 360) # from the above, we know that there is only 1 band, so we can convert the data to a 2d array data = data.squeeze() remove all pixels with no data valid_rows, valid_cols = np.nonzero(data != dataset.get_nodatavals()) valid_data_vals = data[(valid_rows, valid_cols)] plt_data = np.zeros_like(data) plt_data[(valid_rows, valid_cols)] = valid_data_vals f, ax = plt.subplots(figsize=(12, 8)) ax1 = ax.imshow(np.log10(plt_data+0.001), cmap=\u0026#39;pink\u0026#39;) f.colorbar(ax1, label=f\u0026#34;log10(population density)\u0026#34;); Convert pixel positions to coordinates: # covert rows, cols to coordinates: valid_lngs, valid_lats = rasterio.transform.xy(dataset.transform, valid_rows, valid_cols) output_dataframe = pd.DataFrame( data = dict( lat=valid_lats, lng = valid_lngs, population = valid_data_vals ) ) output_dataframe.sample(frac=1.).head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } \u0026lt;svg xmlns=\u0026ldquo;http://www.w3.org/2000/svg\u0026quot; height=\u0026ldquo;24px\u0026quot;viewBox=\u0026ldquo;0 0 24 24\u0026rdquo; width=\u0026ldquo;24px\u0026rdquo;\u0026gt; \u0026lt;script\u0026gt; const buttonEl = document.querySelector('#df-e5bdb33b-903f-4a08-a4cf-9f0c6dceae91 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-e5bdb33b-903f-4a08-a4cf-9f0c6dceae91'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '\u0026lt;a target=\u0026quot;_blank\u0026quot; href=https://colab.research.google.com/notebooks/data_table.ipynb\u0026gt;data table notebook\u0026lt;/a\u0026gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } \u0026lt;/script\u0026gt; \u0026lt;/div\u0026gt; Add lat lng values in web mercator:\n(note that it is a good idea to always_xy param to be true, this means the order of the output is always in the coordinates\u0026rsquo; equivalent to longitude, latitude. There are CRS that can end up swapping the x, y axis from the source CRS)\ndataset.crs CRS.from_epsg(4326) transformer = pyproj.Transformer.from_crs(\u0026#34;EPSG:4326\u0026#34;, \u0026#34;EPSG:3857\u0026#34;, always_xy=True) lng_3857, lat_3857 = transformer.transform(output_dataframe.lng.values.squeeze(), output_dataframe.lat.values.squeeze()) output_dataframe[\u0026#39;lng_3857\u0026#39;] = lng_3857 output_dataframe[\u0026#39;lat_3857\u0026#39;] = lat_3857 output_dataframe.sample(frac=1.).head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } \u0026lt;svg xmlns=\u0026ldquo;http://www.w3.org/2000/svg\u0026quot; height=\u0026ldquo;24px\u0026quot;viewBox=\u0026ldquo;0 0 24 24\u0026rdquo; width=\u0026ldquo;24px\u0026rdquo;\u0026gt; # verify and sanity check output_dataframe.isnull().sum() lat 0 lng 0 population 0 lng_3857 0 lat_3857 0 dtype: int64 ","permalink":"https://wwymak.github.io/posts/2023-01-15-rasterio-tips-and-tricks/","summary":"There are two main ways geospatial data are stored\nrasters, where each \u0026lsquo;pixel\u0026rsquo; stores data values. This corresponds to files such as geotiffs. vectors, where information is stored more like a table form, and each row will have a \u0026lsquo;geometry\u0026rsquo; field which stores information that allows you to recreate a geographical feature, such as a point , a polygon etc. It might also store the coordinate reference system so you can correctly place the shapes on a map This set of notes corresponds to handling raster data.","title":"Geo raster data parsing with rasterio"},{"content":"If you\u0026rsquo;ve got a GPU on your system that you want to run your deep learning model on, you\u0026rsquo;d probably want to check that the library is able to access the GPU. Installation issues/ incorrect setups etc can mean that it\u0026rsquo;s actually inaccessible. I have googled far too many times \u0026lsquo;is tensorflow/pytorch accessing the GPU\u0026rsquo;, so putting this down here so I don\u0026rsquo;t have to go through the same stackoverflow posts again and again 😅\nTensorflow (as of version 2.7) import tensorflow as tf assert len(tf.config.list_physical_devices(\u0026#39;GPU\u0026#39;)) \u0026gt; 0 # if your system has GPU(s), tf.config.list_physical_devices(\u0026#39;GPU\u0026#39;) will return a list like so: # [PhysicalDevice(name=\u0026#39;/physical_device:GPU:0\u0026#39;, device_type=\u0026#39;GPU\u0026#39;), ...] # get cuda version-- if you have installed cudatoolkit using conda in your env, # rather than using the global cuda, you can use this to # check the cuda version is what you expect sys_details = tf.sysconfig.get_build_info() cuda_version = sys_details[\u0026#34;cuda_version\u0026#34;] print(cuda_version) # get cudnn version-- if you have installed cudatoolkit using conda in your env, # rather than using the global cudnn, you can # check this version is what you expect cudnn_version = sys_details[\u0026#34;cudnn_version\u0026#34;] print(cudnn_version) Pytorch (as of version 1.10) import torch # this should print True if the GPU is accessible print(torch.cuda.is_available()) Of course, one of the best way to avoid installation issues with a gpu setup and tensorflow/pytorch is using conda. Unfortunately tensorflow is not \u0026lsquo;officially\u0026rsquo; available on conda, and you might not get latest version, and installing extra tensorflow related libraries that are only available through pip such as tensorflow-io could mean the conda installed version of tensorflow that is linked to the cudatoolkit is uninstalled\u0026hellip;\n","permalink":"https://wwymak.github.io/posts/2021-11-13-tf-check-gpu/","summary":"If you\u0026rsquo;ve got a GPU on your system that you want to run your deep learning model on, you\u0026rsquo;d probably want to check that the library is able to access the GPU. Installation issues/ incorrect setups etc can mean that it\u0026rsquo;s actually inaccessible. I have googled far too many times \u0026lsquo;is tensorflow/pytorch accessing the GPU\u0026rsquo;, so putting this down here so I don\u0026rsquo;t have to go through the same stackoverflow posts again and again 😅","title":"How to check if your deep learning library is actually using the GPU"},{"content":"In BigQuery, there is the concept of repeated fields and arrays. I was trying to figure out how to count how many entries in a table contain a certain value in that repeated column. And since the syntax [value] IN some_array is not valid, there is one extra that is needed.\nSuppose we have some data around freemium podcast platform. In this platform users can pay for an upgraded service, or listen for free. The users can pay in multiple different ways (e.g. they might subscribe to only certain podcasts, or pay for a \u0026rsquo;everything\u0026rsquo; service). In this first scenario, we have a Users table, with various information about them, the most relevant being user_id (of type string) and subscriptions (repeated array of strings corresponding to names of podcasts or unlimited), and we would like to count users who can access the subscribers only content on the podcasts science weekly and magic101.\nThis can be done as follows:\nWITH EligibilityCounts AS ( SELECT user_id, COUNTIF( sub IN (\u0026#39;science weekly\u0026#39;, \u0026#39;magic101\u0026#39;, \u0026#39;unlimited\u0026#39;)) AS paying_member, FROM `Users` u LEFT JOIN UNNEST(u.subscriptions) sub GROUP BY user_id ) SELECT * FROM EligibilityCounts WHERE paying_member \u0026gt; 0 ","permalink":"https://wwymak.github.io/posts/2021-11-11-bigquery-tips-array-counts/","summary":"In BigQuery, there is the concept of repeated fields and arrays. I was trying to figure out how to count how many entries in a table contain a certain value in that repeated column. And since the syntax [value] IN some_array is not valid, there is one extra that is needed.\nSuppose we have some data around freemium podcast platform. In this platform users can pay for an upgraded service, or listen for free.","title":"How to count occurences in Bigquery Array/Repeated Fields"},{"content":"So, I\u0026rsquo;ve decided to revamp my blog, and rather than going with medium, thought that it\u0026rsquo;s a bit nicer to use my own site as the content remains under my control and I can use markdown. Github has a good set of documentation on setting up a github page, but as I am using Hugo as the static site generator, there are a few extra things I need to add. These are the steps I used to get it all up and running. Maybe someone will find them useful, if not, they\u0026rsquo;ll remind me in the future 👼\n1. Create a personal github pages repo Create a new repo with the name \u0026lt;username\u0026gt;.github.io. For now, best to keep the repo empty, otherwise git might complain later when you try to push your local site code to github.\n2. Setup Hugo If you haven\u0026rsquo;t got Hugo installed already, install it now. On ubuntu, this is really easy since Hugo is available in the software centre. (or you can do sudo apt-get hugo). Next, create a new site with hugo on your local machine with hugo new site \u0026lt;username\u0026gt;.github.io \u0026ndash; this will set up a new folder on your machine with name .github.io (you can use whatever name you want for the folder, but I find it a bit easier to make sure repo names always agree with folder names). init the folder with git and add the remote as origin (cd \u0026lt;username\u0026gt;.github.io \u0026amp;\u0026amp; git init \u0026amp;\u0026amp; git remote add origin git@github.com:\u0026lt;username\u0026gt;/\u0026lt;username\u0026gt;.github.io.git) add a theme\u0026ndash; there\u0026rsquo;s some to select from https://themes.gohugo.io/. Pick one you like, and install it (e.g. git submodule add xxxxx) customize your site config.toml file (there are theme specific stuff options that the theme would have documented, the base hugo ones are here) 3. Setup github actions to automatically deploy your blog In the top level, add a .github folder and a workflows folder inside \u0026ndash; this will store the yaml files that defines tasks github should run when there are updates to the code. Here, I am using two handy github actions: https://github.com/peaceiris/actions-gh-pages and https://github.com/peaceiris/actions-hugo. (there are other interesting things you can do with github actions\u0026ndash; more info from the official docs)\nMy page deploy workflow is as follows\nname: hugo publish # deploy only on publish to main branch on: push: branches: - main jobs: build-deploy: runs-on: ubuntu-20.04 # check out repo steps: - uses: actions/checkout@v2 with: submodules: true fetch-depth: 0 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;latest\u0026#39; #minify the code for the site - name: Build run: hugo --minify # deploy website code to the gh-pages branch , ie github pages will serve the site from gh-pages branch -- you can specify a different branch for this - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: ${{ github.ref == \u0026#39;refs/heads/main\u0026#39; }} with: # you don#t need to define the GITHUB_TOKEN, it\u0026#39;s set automatically by github actions github_token: ${{ secrets.GITHUB_TOKEN }} # the code in ./public is what is pushed to the gh-pages branch publish_dir: ./public emptyCommits: true commitMessage: ${{ github.event.head_commit.message }} 4. Tweak the settings on your repo so the site deploys correctly Go to the github pages settings under your repo settings (e.g. https://github.com/\u0026lt;username\u0026gt;/\u0026lt;username\u0026gt;.github.io/settings/pages) and make sure the branch for the site source is correct. (see image below). On the first push the site takes some time to be published. When this is done you\u0026rsquo;ll see the green tick saying your site is published at xxxx\n5. Inspect your site locally Run hugo server and you should see a local version of your site at http://localhost:1313/ (the cli should tell you the correct localhost port). Now is the time to add customisations as you want. For example, if you find the theme doesn\u0026rsquo;t suit, you can delete the theme folder and install a new one. You might want also want to add a favicon etc.\n6. Push your code to github Once you\u0026rsquo;ve push to the main branch, the gh pages actions we set up earlier will run and after \u0026lt; 30mins your blog should be up and running on xxx.github.io 😁\nTips to create a new post, use hugo new blog_post_name.md \u0026ndash; this will automatically create the correct file under /content for your blog remember to change checkt the draft: section in the header\u0026ndash; if this is set to true the default is for Hugo not to render it if you want to use emojis, you need to set enableEmoji = true in your hugo config file for images, you should put them under the static folder, and reference them as ![](image_filename.png) if you want to set up the blog on a custom domain like I\u0026rsquo;ve done here, there are some additional steps to do on both github and your domain name provider\u0026ndash; I will document this in a future post ","permalink":"https://wwymak.github.io/posts/2021-11-05-blog-setup-hugo-github-pages/","summary":"So, I\u0026rsquo;ve decided to revamp my blog, and rather than going with medium, thought that it\u0026rsquo;s a bit nicer to use my own site as the content remains under my control and I can use markdown. Github has a good set of documentation on setting up a github page, but as I am using Hugo as the static site generator, there are a few extra things I need to add. These are the steps I used to get it all up and running.","title":"Blog Setup With Hugo and Github Pages"}]