[{"content":" This year lunar new year is on the 22nd Jan, and as it nicely coincides with @sundayafternoonbakingclub penaut butter theme, I made these bite sized peanut cookies as they are popular as new year treats (or rather, more popular than usual\u0026ndash; I am fairly sure you can get them year round).\nThey are really easy to make as I took the shortcut and made them with PipandNut crunchy peanut butter \u0026ndash; you more or less peanut butter, oil, flour and sugar together, shape, and bake with an egg yolk egg wash.\nAdmittedly I don\u0026rsquo;t recall being very fond of chinese new year foods when I was growing up, so I don\u0026rsquo;t have a baseline to judge these against, but I definitely find them really delicious, with a very peanutty flavour 😊\nIngredients 180g flour 80g icing sugar 1tsp salt 100g crunchy peanut butter (not sweetened) 80g olive oil 2 medium egg yolks (for egg wash \u0026ndash; uk medium sized ) Baking notes it\u0026rsquo;s definitely worth sifting the flour and icing sugar to reduce lumps for bit sized cookies, they\u0026rsquo;re around 10g each. For more substance, double the size recipe origin From foodiebaker\n","permalink":"https://wwymak.github.io/bakery/2023-01-22-chinese-peanut-cookies/","summary":"This year lunar new year is on the 22nd Jan, and as it nicely coincides with @sundayafternoonbakingclub penaut butter theme, I made these bite sized peanut cookies as they are popular as new year treats (or rather, more popular than usual\u0026ndash; I am fairly sure you can get them year round).\nThey are really easy to make as I took the shortcut and made them with PipandNut crunchy peanut butter \u0026ndash; you more or less peanut butter, oil, flour and sugar together, shape, and bake with an egg yolk egg wash.","title":"Chinese Peanut Cookies"},{"content":"There are two main ways geospatial data are stored\nrasters, where each \u0026lsquo;pixel\u0026rsquo; stores data values. This corresponds to files such as geotiffs. vectors, where information is stored more like a table form, and each row will have a \u0026lsquo;geometry\u0026rsquo; field which stores information that allows you to recreate a geographical feature, such as a point , a polygon etc. It might also store the coordinate reference system so you can correctly place the shapes on a map This set of notes corresponds to handling raster data. For this, we will make use of the rasterio library for manipulating the data, as well as pyproj for coordinate transforms.\n(You can access a running version of this notebook on colab here)\nSetup For this demo, I will be using a population dataset from NASA \u0026ldquo;Gridded Population of the World, Version 4 (GPWv4): Population Count Adjusted to Match 2015 Revision of UN WPP Country Totals\u0026rdquo;\n!pip install rasterio pyproj --quiet import rasterio import pyproj import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline peak at data: asc is much like a text file and we can use conventional text handling tools and a bit of pandas to parse it, but it is much less mistake prone if we use rasterio to handle it\n!cat gpw_v4_population_count_adjusted_to_2015_unwpp_country_totals_rev11_2020_1_deg.asc | head -6 ncols 360 nrows 180 xllcorner -180 yllcorner -90 cellsize 1.0000000000001 NODATA_value -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 ... !cat gpw_v4_population_count_adjusted_to_2015_unwpp_country_totals_rev11_2020_1_deg.prj | head -10 Projection GEOGRAPHIC Datum WGS84 Spheroid WGS84 Units DD Zunits NO Parameters parsing the data For this example, we would like to convert the raster to a tabular form we can store in a \u0026lsquo;conventional\u0026rsquo; database/csv file. For this, we will do these tasks:\nread the file into memory\nfilter out any pixels/cells with no data\nconvert the data into a \u0026rsquo;long\u0026rsquo; table with the format longitude | latitude | population | obtain the lat/lng values in web mercator coordinates\nRead the file: note that rasterio can also open files on certain cloud providers, such as GCS/S3 etc, just pass it the relevant filepath in gs://, s3:// format\ndataset = rasterio.open(\u0026#34;gpw_v4_population_count_adjusted_to_2015_unwpp_country_totals_rev11_2020_1_deg.asc\u0026#34;) the dataset object holds metadata:\n# extent of the data and crs -- note we are getting the CRS info from the .prj file also supplied dataset.bounds, dataset.crs (BoundingBox(left=-180.0, bottom=-90.0, right=180.0, top=90.0), CRS.from_epsg(4326)) The actual values of the pixels are in the 1st \u0026lsquo;band\u0026rsquo;. There can be more than 1 band\u0026ndash; for example satellite imagery can comes in \u0026gt; 10 bands. We can either read all the bands in at once, or one by one. (Note that the indexing starts with 1 for the bands if reading in one by one)\ndata= dataset.read() data.shape (1, 180, 360) # from the above, we know that there is only 1 band, so we can convert the data to a 2d array data = data.squeeze() remove all pixels with no data valid_rows, valid_cols = np.nonzero(data != dataset.get_nodatavals()) valid_data_vals = data[(valid_rows, valid_cols)] plt_data = np.zeros_like(data) plt_data[(valid_rows, valid_cols)] = valid_data_vals f, ax = plt.subplots(figsize=(12, 8)) ax1 = ax.imshow(np.log10(plt_data+0.001), cmap=\u0026#39;pink\u0026#39;) f.colorbar(ax1, label=f\u0026#34;log10(population density)\u0026#34;); Convert pixel positions to coordinates: # covert rows, cols to coordinates: valid_lngs, valid_lats = rasterio.transform.xy(dataset.transform, valid_rows, valid_cols) output_dataframe = pd.DataFrame( data = dict( lat=valid_lats, lng = valid_lngs, population = valid_data_vals ) ) output_dataframe.sample(frac=1.).head() index lat lng population 6506 53.5 124.5 9.351563e+03 1035 75.5 141.5 1.780356e+02 3034 66.5 72.5 1.972377e+03 7488 48.5 -113.5 9.566622e+03 10740 32.5 116.5 4.994749e+06 Add lat lng values in web mercator:\n(note that it is a good idea to always_xy param to be true, this means the order of the output is always in the coordinates\u0026rsquo; equivalent to longitude, latitude. There are CRS that can end up swapping the x, y axis from the source CRS)\ndataset.crs CRS.from_epsg(4326) transformer = pyproj.Transformer.from_crs(\u0026#34;EPSG:4326\u0026#34;, \u0026#34;EPSG:3857\u0026#34;, always_xy=True) lng_3857, lat_3857 = transformer.transform(output_dataframe.lng.values.squeeze(), output_dataframe.lat.values.squeeze()) output_dataframe[\u0026#39;lng_3857\u0026#39;] = lng_3857 output_dataframe[\u0026#39;lat_3857\u0026#39;] = lat_3857 output_dataframe.sample(frac=1.).head() index lat lng population lng_3857 lat_3857 11941 25.5 124.5 2.870859e+01 1.385928e+07 2.937284e+06 17692 -22.5 117.5 4.557170e+03 1.308004e+07 -2.571663e+06 44 82.5 -77.5 3.311810e-02 -8.627261e+06 1.738069e+07 9618 38.5 46.5 1.455383e+06 5.176356e+06 4.650302e+06 16594 -12.5 122.5 1.345766e-03 1.363664e+07 -1.402665e+06 # verify and sanity check output_dataframe.isnull().sum() lat 0 lng 0 population 0 lng_3857 0 lat_3857 0 dtype: int64 ","permalink":"https://wwymak.github.io/tech/2023-01-15-rasterio-tips-and-tricks/","summary":"There are two main ways geospatial data are stored\nrasters, where each \u0026lsquo;pixel\u0026rsquo; stores data values. This corresponds to files such as geotiffs. vectors, where information is stored more like a table form, and each row will have a \u0026lsquo;geometry\u0026rsquo; field which stores information that allows you to recreate a geographical feature, such as a point , a polygon etc. It might also store the coordinate reference system so you can correctly place the shapes on a map This set of notes corresponds to handling raster data.","title":"Geo raster data parsing with rasterio"},{"content":"Introduction The census provides a rich set of demographic information that could be useful for various data science tasks, such as geomarketing, house price analysis etc. With the results of the 2021 census being published recently, this notebook demonstrates how to manipulate this data with FOSS python tools to build a dataset that can be used for downstream tasks.\nFor this demo, I will be focusing on output areas (OA) of London:\ndownload the boundaries for these OA download demographics datasets (population, household size, gender, education level) convert the irregular geometries of the OA into 100m by 100m cells calculate the avgerages for the demographics per bin convert the cell data into raster These are the packages we will need:\nimport pandas as pd import geopandas as gpd from shapely.geometry import Polygon import numpy as np from tqdm import tqdm import fiona import rasterio as rio from geocube.api.core import make_geocube Datasets Population data for Census 2021 according to Output Area (https://www.ons.gov.uk/filters/b9532b29-299e-4a23-9fc8-b99d68e172b9/dimensions)\nTitle: Population density Description: This dataset provides Census 2021 estimates that classify usual residents in England and Wales by population density (number of usual residents per square kilometre). The estimates are as at Census Day, 21 March 2021.\nOutput Areas (OAs) are the lowest level of geographical area for census statistics and were first created following the 2001 Census. Each OA is made up of between 40 and 250 households and a usually resident population of between 100 and 625 persons and may change after each census.\npopulation_oa = pd.read_csv(\u0026#39;data/uk-census-2021-oa.csv\u0026#39;) population_oa.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Geographic manipulation\u0026ndash; OA boundaries to cells To match the above OA codes with actual geography, we have to obtain the boundaries of each output area\u0026ndash; these can be found on UK\u0026rsquo;s goverment geoportal\u0026ndash; note that I am using the \u0026lsquo;generalised\u0026rsquo; version instead of the full version.\nboundaries = gpd.read_file(\u0026#39;data/Output_Areas_(Dec_2021)_Boundaries_Generalised_Clipped_EW_(BGC)/Output_Areas_(Dec_2021)_Boundaries_Generalised_Clipped_EW_(BGC).shp\u0026#39;) boundaries.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } In the boundaries geodataframe, the columns we\u0026rsquo;ll need is that \u0026lsquo;OA21CD\u0026rsquo;, \u0026lsquo;Shape_Are\u0026rsquo; and \u0026lsquo;geometry columns\u0026rsquo;: \u0026lsquo;OA21CD\u0026rsquo; can join to the population dataset from above, Shape__Are gives the area of the OA in m^2 (note that this is different from the poulation measurement unit, which is in km^2). We can in fact cross check it against the geometry column\n# check that the projection is indeed british national grid eastings and northings print(f\u0026#34;geodata has projection of {boundaries.crs}\u0026#34;) # check that the geometry gives ~the same area as what shape_are gives to a 10m^2 tolerance assert ((boundaries.area.astype(int)- boundaries.Shape__Are.astype(int)).abs() \u0026lt; 10).sum() == len(boundaries) geodata has projection of epsg:27700 Since we are only interested in Greater London, we can filter out rows from the boundaries dataframe that are outside of the london bounding box.\nWe can get the bounding box of london using the OSM query https://nominatim.openstreetmap.org/search.php?city=london\u0026amp;country=uk\u0026amp;format=jsonv2 (the first entry is the correct one), giving a boundingbox of ymin, ymax, xmin, xmax of \u0026ldquo;51.2867602\u0026rdquo;,\u0026ldquo;51.6918741\u0026rdquo;,\u0026quot;-0.5103751\u0026quot;,\u0026ldquo;0.3340155\u0026rdquo;\nldn_bounding_box = Polygon.from_bounds( -0.5103751, 51.2867602,0.3340155, 51.691874 ) ldn_bounding_box = gpd.GeoSeries([ldn_bounding_box], crs=\u0026#39;epsg:4326\u0026#39;).to_crs(boundaries.crs) ldn_bounding_box.geometry 0 POLYGON ((503976.311 155234.131, 503059.759 20... dtype: geometry Convert the geodataframe into a grid for downstream tasks\u0026ndash; Ordnance survey actually has a file of british grids at various resolutions prebuilt (https://github.com/OrdnanceSurvey/OS-British-National-Grids), \u0026ndash; the smallest is 1km by 1km, so we will construct our own 100m x 100m grid for london (We can do it for the whole uk, but we will have to iterate through the dataset as the number of cells will become very large)\ndiversion\nThis demos how to use the grid gpkg file from OS\u0026ndash; the gpkg is a vector file with multiple \u0026rsquo;layers\u0026rsquo; the same way a raster file can have bands\n# get the layer names list(fiona.listlayers(\u0026#39;data/os_bng_grids.gpkg\u0026#39;)) ['100km_grid', '50km_grid', '20km_grid', '10km_grid', '5km_grid', '1km_grid'] layername = \u0026#39;1km_grid\u0026#39; bsng_grid_1km = gpd.read_file(\u0026#39;data/os_bng_grids.gpkg\u0026#39;, layer=layername) bsng_grid_1km.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Back to our task, we filter out the OA that intersects the london bounding box:\ngpd.GeoDataFrame(ldn_bounding_box.reset_index()).geometry 0 POLYGON ((503976.311 155234.131, 503059.759 20... Name: 0, dtype: geometry ldn_boundaries = boundaries[[\u0026#39;OA21CD\u0026#39;, \u0026#39;geometry\u0026#39;]].sjoin(gpd.GeoDataFrame(ldn_bounding_box.reset_index()), how=\u0026#39;inner\u0026#39;)[[\u0026#39;OA21CD\u0026#39;, \u0026#39;geometry\u0026#39;]] ldn_boundaries.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ldn_bounding_box.geometry[0].bounds (503059.75944664044, 155234.13110756868, 562854.9611320551, 201813.5769950911) # create a 100m by 100m grid for london cellsize=100 # British National Grid extent (0,0 700000,1300000) xmin, ymin, xmax, ymax = [100 * (x//100 )for x in ldn_bounding_box.geometry[0].bounds] matrix = np.mgrid[int(xmin):int(xmax + cellsize):cellsize, int(ymin):int(ymax+ cellsize):cellsize] xcoor = matrix[0].flatten() ycoor = matrix[1].flatten() xcoor_max = xcoor +cellsize ycoor_max = ycoor + cellsize polygons = [] for x, y, xmax, ymax in tqdm(zip (xcoor, ycoor, xcoor_max, ycoor_max), total=len(ycoor)): polygons.append(Polygon.from_bounds(x, y, xmax, ymax)) grid = gpd.GeoDataFrame(data={\u0026#39;cellid\u0026#39;: np.arange(len(polygons))},geometry=polygons, crs=boundaries.crs) 100%|████████████████████████████████| 279733/279733 [00:04\u0026lt;00:00, 65379.51it/s] Join the grid to the OA polygons to get the demographics info per grid cell\nprint(grid.shape) print(ldn_boundaries.shape) ldn_demographics_grid = grid.sjoin(boundaries) (279733, 2) (30160, 2) ldn_demographics_grid .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # as we can see, the OA size span is quite large --luckily most OA are larger than a grid cell. ax =ldn_demographics_grid.drop_duplicates(subset=\u0026#39;OA21CD\u0026#39;).Shape__Are.apply(np.log10).hist(bins = 100) ax.set_xlabel(\u0026#39;OA area m^2\u0026#39;) ax.set_ylabel(\u0026#39;number of OA\u0026#39;) ax.vlines(4,0,1800, colors=\u0026#39;b\u0026#39;, label=\u0026#39;10,000 m^2 (Size of cell)\u0026#39;) ax.legend() \u0026lt;matplotlib.legend.Legend at 0x7f8b0b33e680\u0026gt; There are clearly duplicates\u0026ndash; e.g. when a grid cell span more than 1 OA, and where 1OA span more than 1cell\u0026ndash; this is more likely for the OA to span mutliple cells than the other way round. Base on this assumption, we assign each cell to one OA by dropping duplicadtes\nldn_demographics_grid = ldn_demographics_grid.drop_duplicates(subset=\u0026#39;cellid\u0026#39;) # drop all the unneed cols ldn_demographics_grid = ldn_demographics_grid[[\u0026#39;cellid\u0026#39;, \u0026#39;OA21CD\u0026#39;, \u0026#39;geometry\u0026#39;]] Finally, let\u0026rsquo;s populate these grid cells with demographic data\nhousehold size by OA education [population density]((https://www.ons.gov.uk/filters/b9532b29-299e-4a23-9fc8-b99d68e172b9/dimensions) sex We have discussed the population data above, let\u0026rsquo;s now tidy up the other datasets\nhousehold size houshold = pd.read_csv(\u0026#39;data/household_size_output_area_census2021.csv\u0026#39;) houshold.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } We can use the \u0026lsquo;code\u0026rsquo; as a proxy for household size\nhoushold[[\u0026#39;Household size (9 categories)\u0026#39;, \u0026#39;Household size (9 categories) Code\u0026#39;]].drop_duplicates() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_household_size = houshold[[\u0026#39;Output Areas Code\u0026#39;, \u0026#39;Household size (9 categories) Code\u0026#39;,\u0026#39;Observation\u0026#39;]] mean_household_size[\u0026#39;counts\u0026#39;] = mean_household_size[\u0026#39;Household size (9 categories) Code\u0026#39;] * mean_household_size[\u0026#39;Observation\u0026#39;] mean_household_size = mean_household_size.groupby(\u0026#39;Output Areas Code\u0026#39;)[\u0026#39;counts\u0026#39;].sum() mean_household_size_totals = houshold[[\u0026#39;Output Areas Code\u0026#39;, \u0026#39;Household size (9 categories) Code\u0026#39;,\u0026#39;Observation\u0026#39;]].groupby(\u0026#39;Output Areas Code\u0026#39;)[\u0026#39;Observation\u0026#39;].sum() /tmp/ipykernel_28838/2961719914.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy mean_household_size['counts'] = mean_household_size['Household size (9 categories) Code'] * mean_household_size['Observation'] mean_household_size.reset_index() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_household_size = mean_household_size.reset_index().merge(mean_household_size_totals.reset_index(), on=\u0026#39;Output Areas Code\u0026#39;) mean_household_size[\u0026#39;avg_household_size\u0026#39;] = mean_household_size[\u0026#39;counts\u0026#39;] / mean_household_size[\u0026#39;Observation\u0026#39;] mean_household_size.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } gender gender = pd.read_csv(\u0026#39;data/uk-census-2021-sex-oa.csv\u0026#39;) gender.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } females = gender[gender[\u0026#39;Sex (2 categories)\u0026#39;] == \u0026#39;Female\u0026#39;] gender_total = gender.groupby([\u0026#39;Output Areas Code\u0026#39;])[\u0026#39;Observation\u0026#39;].sum().reset_index() percent_female = females[[\u0026#39;Output Areas\u0026#39;, \u0026#39;Observation\u0026#39;]].merge(gender_total, left_on=\u0026#39;Output Areas\u0026#39;, right_on=\u0026#39;Output Areas Code\u0026#39;) percent_female[\u0026#39;pct\u0026#39;] = percent_female[\u0026#39;Observation_x\u0026#39;] / percent_female[\u0026#39;Observation_y\u0026#39;] percent_female.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } education education = pd.read_csv(\u0026#39;data/uk-census-2021-education-oa.csv\u0026#39;) education.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } education[[\u0026#39;Highest level of qualification (8 categories) Code\u0026#39;,\u0026#39;Highest level of qualification (8 categories)\u0026#39;\t]].drop_duplicates() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } here make the simplifying assumption that we ignore the \u0026lsquo;does not apply category\u0026rsquo;, and the categories code go up in more advanced education\nmean_education_level = education[[\u0026#39;Output Areas Code\u0026#39;, \u0026#39;Highest level of qualification (8 categories) Code\u0026#39;,\u0026#39;Observation\u0026#39;]] mean_education_level[\u0026#39;counts\u0026#39;] = mean_education_level[\u0026#39;Highest level of qualification (8 categories) Code\u0026#39;].clip(lower=0) * mean_education_level[\u0026#39;Observation\u0026#39;] mean_education_level = mean_education_level.groupby(\u0026#39;Output Areas Code\u0026#39;)[\u0026#39;counts\u0026#39;].sum() mean_education_level_totals = education[[\u0026#39;Output Areas Code\u0026#39;, \u0026#39;Highest level of qualification (8 categories) Code\u0026#39;,\u0026#39;Observation\u0026#39;]].groupby(\u0026#39;Output Areas Code\u0026#39;)[\u0026#39;Observation\u0026#39;].sum() mean_education_level = mean_education_level.reset_index().merge(mean_education_level_totals.reset_index(), on=\u0026#39;Output Areas Code\u0026#39;) mean_education_level[\u0026#39;avg_education_level\u0026#39;] = mean_education_level[\u0026#39;counts\u0026#39;] / mean_education_level[\u0026#39;Observation\u0026#39;] mean_education_level.head() /tmp/ipykernel_28838/566605148.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy mean_education_level['counts'] = mean_education_level['Highest level of qualification (8 categories) Code'].clip(lower=0) * mean_education_level['Observation'] .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # add education level info ldn_demographics_grid_processed = ldn_demographics_grid.merge(mean_education_level[[ \u0026#39;Output Areas Code\u0026#39;, \u0026#39;avg_education_level\u0026#39;]], left_on=\u0026#39;OA21CD\u0026#39;, right_on=\u0026#39;Output Areas Code\u0026#39;).drop(columns=\u0026#39;Output Areas Code\u0026#39;) # add % female ldn_demographics_grid_processed = ldn_demographics_grid_processed.merge(percent_female[[ \u0026#39;Output Areas Code\u0026#39;, \u0026#39;pct\u0026#39;]], left_on=\u0026#39;OA21CD\u0026#39;, right_on=\u0026#39;Output Areas Code\u0026#39;).drop(columns=\u0026#39;Output Areas Code\u0026#39;).rename(columns={\u0026#39;pct\u0026#39;: \u0026#39;pct_females\u0026#39;}) # add mean household size ldn_demographics_grid_processed = ldn_demographics_grid_processed.merge(mean_household_size[[ \u0026#39;Output Areas Code\u0026#39;, \u0026#39;avg_household_size\u0026#39;]], left_on=\u0026#39;OA21CD\u0026#39;, right_on=\u0026#39;Output Areas Code\u0026#39;).drop(columns=\u0026#39;Output Areas Code\u0026#39;) ldn_demographics_grid_processed = ldn_demographics_grid_processed.merge(population_oa[[ \u0026#39;Output Areas Code\u0026#39;, \u0026#39;Observation\u0026#39;]], left_on=\u0026#39;OA21CD\u0026#39;, right_on=\u0026#39;Output Areas Code\u0026#39;).drop(columns=\u0026#39;Output Areas Code\u0026#39;).rename( columns={\u0026#39;Observation\u0026#39;: \u0026#39;pop_density\u0026#39;}) ldn_demographics_grid_processed[\u0026#39;popoulation\u0026#39;] = ldn_demographics_grid_processed[\u0026#39;pop_density\u0026#39;] * (100 * 100)/(1000 * 1000) ldn_demographics_grid_processed.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ldn_demographics_grid_processed.to_file(\u0026#39;data/ldn_demographics_grid_processed.shp\u0026#39;) /tmp/ipykernel_28838/324827064.py:1: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile. ldn_demographics_grid_processed.to_file('data/ldn_demographics_grid_processed.shp') Rasterizing geodataframe Certain tools work better with raster data, and since we have converted our data from a irregular geometry into cells, we can go all the way and convert the vector data into raster, using geocube.\nCapping/binning population values\nThe population values have a fairly long tail\u0026ndash; let\u0026rsquo;s cap this so it is easier to plot and handle\n# printing out some stats -- we can see 95% cells have \u0026lt; 150 people ldn_demographics_grid_processed[\u0026#39;popoulation\u0026#39;].quantile([0.2, 0.4, 0.6, 0.8]),ldn_demographics_grid_processed[\u0026#39;popoulation\u0026#39;].quantile([0.05]), ldn_demographics_grid_processed[\u0026#39;popoulation\u0026#39;].quantile([0.95]), ldn_demographics_grid_processed[\u0026#39;popoulation\u0026#39;].min(), ldn_demographics_grid_processed[\u0026#39;popoulation\u0026#39;].max() (0.2 1.024 0.4 4.157 0.6 21.138 0.8 62.500 Name: popoulation, dtype: float64, 0.05 0.321 Name: popoulation, dtype: float64, 0.95 130.5817 Name: popoulation, dtype: float64, 0.156, 2393.333) ldn_demographics_grid_processed[\u0026#39;popoulation\u0026#39;] = ldn_demographics_grid_processed[\u0026#39;popoulation\u0026#39;].clip(lower=0, upper=150) ldn_demographics_grid_processed[\u0026#39;popoulation\u0026#39;].max() 150.0 Now we convert the vector to raster (the resolution is what we are using for each of our cells\u0026ndash; 100m )\nldn_demographics_raster = make_geocube( vector_data=ldn_demographics_grid_processed, measurements=[\u0026#34;popoulation\u0026#34;, \u0026#34;avg_education_level\u0026#34;, \u0026#34;pct_females\u0026#34;], resolution=(-100, 100), ) ldn_demographics_raster[\u0026#39;popoulation\u0026#39;].plot() \u0026lt;matplotlib.collections.QuadMesh at 0x7f8adde759c0\u0026gt; ldn_demographics_raster[\u0026#39;pct_females\u0026#39;].plot() \u0026lt;matplotlib.collections.QuadMesh at 0x7f8b0c795db0\u0026gt; ldn_demographics_raster[\u0026#39;avg_education_level\u0026#39;].plot() \u0026lt;matplotlib.collections.QuadMesh at 0x7f8b0d76e2f0\u0026gt; # save our raster data ldn_demographics_raster.rio.to_raster(\u0026#34;data/ldn_demographics_raster.tiff\u0026#34;) ","permalink":"https://wwymak.github.io/tech/2023-02-01-uk-population-census-2021/","summary":"Introduction The census provides a rich set of demographic information that could be useful for various data science tasks, such as geomarketing, house price analysis etc. With the results of the 2021 census being published recently, this notebook demonstrates how to manipulate this data with FOSS python tools to build a dataset that can be used for downstream tasks.\nFor this demo, I will be focusing on output areas (OA) of London:","title":"Processing UK 2021 census open data with python tools"},{"content":" Olive and sun dried tomato sourdough focaccia for this week’s @sundayafternoonbakingclub focaccia theme. The special sauce in this is my homemade chilli oil infused with chillis, Szechuan peppercorns, star anise and a few other spices. It gave it a nice bit of heat and also loads of flavour from all the other aromatics, and I definitely prefer this to plain olive oil.\nI started off with my basic sourdough focaccia recipe but used a 50-50 mix of white and whole meal bread flour— and also accidentally ended up with a 100% hydration as I sifted out the bran from the wholemeal flour and forgot to compensate for it 😆\nI used a big baking tray this time as the 2nd biggest I used last time was a bit too small, but I think this tray is a big too big— the focaccia is a bit on the flat side! But still wonderfully light and puffy, so I reckoned I did a decent enough job\nIngredients 240g white bread flour 240g wholewheat flour (bran sifted out) 60g semolina 1.25g instant yeast 150g refreshed starter (white) ~100g black olives in brine ~50g sundried tomatoes 30gchilli oil + more as needed for pans etc Baking notes getting the right size pan is key\u0026ndash; this time I used too large a pan and did the dough spread\u0026hellip; It is thin enough you can call it a 🍕 if using non stick cookware can go a bit lighter on the oil that you add to pans using non-stick oil spray is a good option (the ones you buy from a shop has lecithin added to it, which helps the oil spread, especially handy for non stick trays) recipe origin From Wordloaf\n","permalink":"https://wwymak.github.io/bakery/2023-01-06-olive-sundried-tomato-sourdough-focaccia/","summary":"Olive and sun dried tomato sourdough focaccia for this week’s @sundayafternoonbakingclub focaccia theme. The special sauce in this is my homemade chilli oil infused with chillis, Szechuan peppercorns, star anise and a few other spices. It gave it a nice bit of heat and also loads of flavour from all the other aromatics, and I definitely prefer this to plain olive oil.\nI started off with my basic sourdough focaccia recipe but used a 50-50 mix of white and whole meal bread flour— and also accidentally ended up with a 100% hydration as I sifted out the bran from the wholemeal flour and forgot to compensate for it 😆","title":"Olive and sun dried tomato sourdough focaccia"},{"content":"If you\u0026rsquo;ve got a GPU on your system that you want to run your deep learning model on, you\u0026rsquo;d probably want to check that the library is able to access the GPU. Installation issues/ incorrect setups etc can mean that it\u0026rsquo;s actually inaccessible. I have googled far too many times \u0026lsquo;is tensorflow/pytorch accessing the GPU\u0026rsquo;, so putting this down here so I don\u0026rsquo;t have to go through the same stackoverflow posts again and again 😅\nTensorflow (as of version 2.7) import tensorflow as tf assert len(tf.config.list_physical_devices(\u0026#39;GPU\u0026#39;)) \u0026gt; 0 # if your system has GPU(s), tf.config.list_physical_devices(\u0026#39;GPU\u0026#39;) will return a list like so: # [PhysicalDevice(name=\u0026#39;/physical_device:GPU:0\u0026#39;, device_type=\u0026#39;GPU\u0026#39;), ...] # get cuda version-- if you have installed cudatoolkit using conda in your env, # rather than using the global cuda, you can use this to # check the cuda version is what you expect sys_details = tf.sysconfig.get_build_info() cuda_version = sys_details[\u0026#34;cuda_version\u0026#34;] print(cuda_version) # get cudnn version-- if you have installed cudatoolkit using conda in your env, # rather than using the global cudnn, you can # check this version is what you expect cudnn_version = sys_details[\u0026#34;cudnn_version\u0026#34;] print(cudnn_version) Pytorch (as of version 1.10) import torch # this should print True if the GPU is accessible print(torch.cuda.is_available()) Of course, one of the best way to avoid installation issues with a gpu setup and tensorflow/pytorch is using conda. Unfortunately tensorflow is not \u0026lsquo;officially\u0026rsquo; available on conda, and you might not get latest version, and installing extra tensorflow related libraries that are only available through pip such as tensorflow-io could mean the conda installed version of tensorflow that is linked to the cudatoolkit is uninstalled\u0026hellip;\n","permalink":"https://wwymak.github.io/tech/2021-11-13-tf-check-gpu/","summary":"If you\u0026rsquo;ve got a GPU on your system that you want to run your deep learning model on, you\u0026rsquo;d probably want to check that the library is able to access the GPU. Installation issues/ incorrect setups etc can mean that it\u0026rsquo;s actually inaccessible. I have googled far too many times \u0026lsquo;is tensorflow/pytorch accessing the GPU\u0026rsquo;, so putting this down here so I don\u0026rsquo;t have to go through the same stackoverflow posts again and again 😅","title":"How to check if your deep learning library is actually using the GPU"},{"content":"In BigQuery, there is the concept of repeated fields and arrays. I was trying to figure out how to count how many entries in a table contain a certain value in that repeated column. And since the syntax [value] IN some_array is not valid, there is one extra that is needed.\nSuppose we have some data around freemium podcast platform. In this platform users can pay for an upgraded service, or listen for free. The users can pay in multiple different ways (e.g. they might subscribe to only certain podcasts, or pay for a \u0026rsquo;everything\u0026rsquo; service). In this first scenario, we have a Users table, with various information about them, the most relevant being user_id (of type string) and subscriptions (repeated array of strings corresponding to names of podcasts or unlimited), and we would like to count users who can access the subscribers only content on the podcasts science weekly and magic101.\nThis can be done as follows:\nWITH EligibilityCounts AS ( SELECT user_id, COUNTIF( sub IN (\u0026#39;science weekly\u0026#39;, \u0026#39;magic101\u0026#39;, \u0026#39;unlimited\u0026#39;)) AS paying_member, FROM `Users` u LEFT JOIN UNNEST(u.subscriptions) sub GROUP BY user_id ) SELECT * FROM EligibilityCounts WHERE paying_member \u0026gt; 0 ","permalink":"https://wwymak.github.io/tech/2021-11-11-bigquery-tips-array-counts/","summary":"In BigQuery, there is the concept of repeated fields and arrays. I was trying to figure out how to count how many entries in a table contain a certain value in that repeated column. And since the syntax [value] IN some_array is not valid, there is one extra that is needed.\nSuppose we have some data around freemium podcast platform. In this platform users can pay for an upgraded service, or listen for free.","title":"How to count occurences in Bigquery Array/Repeated Fields"},{"content":"So, I\u0026rsquo;ve decided to revamp my blog, and rather than going with medium, thought that it\u0026rsquo;s a bit nicer to use my own site as the content remains under my control and I can use markdown. Github has a good set of documentation on setting up a github page, but as I am using Hugo as the static site generator, there are a few extra things I need to add. These are the steps I used to get it all up and running. Maybe someone will find them useful, if not, they\u0026rsquo;ll remind me in the future 👼\n1. Create a personal github pages repo Create a new repo with the name \u0026lt;username\u0026gt;.github.io. For now, best to keep the repo empty, otherwise git might complain later when you try to push your local site code to github.\n2. Setup Hugo If you haven\u0026rsquo;t got Hugo installed already, install it now. On ubuntu, this is really easy since Hugo is available in the software centre. (or you can do sudo apt-get hugo). Next, create a new site with hugo on your local machine with hugo new site \u0026lt;username\u0026gt;.github.io \u0026ndash; this will set up a new folder on your machine with name .github.io (you can use whatever name you want for the folder, but I find it a bit easier to make sure repo names always agree with folder names). init the folder with git and add the remote as origin (cd \u0026lt;username\u0026gt;.github.io \u0026amp;\u0026amp; git init \u0026amp;\u0026amp; git remote add origin git@github.com:\u0026lt;username\u0026gt;/\u0026lt;username\u0026gt;.github.io.git) add a theme\u0026ndash; there\u0026rsquo;s some to select from https://themes.gohugo.io/. Pick one you like, and install it (e.g. git submodule add xxxxx) customize your site config.toml file (there are theme specific stuff options that the theme would have documented, the base hugo ones are here) 3. Setup github actions to automatically deploy your blog In the top level, add a .github folder and a workflows folder inside \u0026ndash; this will store the yaml files that defines tasks github should run when there are updates to the code. Here, I am using two handy github actions: https://github.com/peaceiris/actions-gh-pages and https://github.com/peaceiris/actions-hugo. (there are other interesting things you can do with github actions\u0026ndash; more info from the official docs)\nMy page deploy workflow is as follows\nname: hugo publish # deploy only on publish to main branch on: push: branches: - main jobs: build-deploy: runs-on: ubuntu-20.04 # check out repo steps: - uses: actions/checkout@v2 with: submodules: true fetch-depth: 0 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;latest\u0026#39; #minify the code for the site - name: Build run: hugo --minify # deploy website code to the gh-pages branch , ie github pages will serve the site from gh-pages branch -- you can specify a different branch for this - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: ${{ github.ref == \u0026#39;refs/heads/main\u0026#39; }} with: # you don#t need to define the GITHUB_TOKEN, it\u0026#39;s set automatically by github actions github_token: ${{ secrets.GITHUB_TOKEN }} # the code in ./public is what is pushed to the gh-pages branch publish_dir: ./public emptyCommits: true commitMessage: ${{ github.event.head_commit.message }} 4. Tweak the settings on your repo so the site deploys correctly Go to the github pages settings under your repo settings (e.g. https://github.com/\u0026lt;username\u0026gt;/\u0026lt;username\u0026gt;.github.io/settings/pages) and make sure the branch for the site source is correct. (see image below). On the first push the site takes some time to be published. When this is done you\u0026rsquo;ll see the green tick saying your site is published at xxxx\n5. Inspect your site locally Run hugo server and you should see a local version of your site at http://localhost:1313/ (the cli should tell you the correct localhost port). Now is the time to add customisations as you want. For example, if you find the theme doesn\u0026rsquo;t suit, you can delete the theme folder and install a new one. You might want also want to add a favicon etc.\n6. Push your code to github Once you\u0026rsquo;ve push to the main branch, the gh pages actions we set up earlier will run and after \u0026lt; 30mins your blog should be up and running on xxx.github.io 😁\nTips to create a new post, use hugo new blog_post_name.md \u0026ndash; this will automatically create the correct file under /content for your blog remember to change checkt the draft: section in the header\u0026ndash; if this is set to true the default is for Hugo not to render it if you want to use emojis, you need to set enableEmoji = true in your hugo config file for images, you should put them under the static folder, and reference them as ![](image_filename.png) if you want to set up the blog on a custom domain like I\u0026rsquo;ve done here, there are some additional steps to do on both github and your domain name provider\u0026ndash; I will document this in a future post ","permalink":"https://wwymak.github.io/tech/2021-11-05-blog-setup-hugo-github-pages/","summary":"So, I\u0026rsquo;ve decided to revamp my blog, and rather than going with medium, thought that it\u0026rsquo;s a bit nicer to use my own site as the content remains under my control and I can use markdown. Github has a good set of documentation on setting up a github page, but as I am using Hugo as the static site generator, there are a few extra things I need to add. These are the steps I used to get it all up and running.","title":"Blog Setup With Hugo and Github Pages"}]